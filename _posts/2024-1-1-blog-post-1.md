---
title: '学生论文读后感分享'
date: 2023-10-22
permalink: /posts/2023/10/blog-post-1/
tags:
  - 论文分享
  - Sigcomm
  - FAST
  - Caching 
---

2024年的第一天，分享本科毕设罗嘉宇同学阅读两篇关于CDN cache的论文读后感，为新的一年开个好兆头。


# 12.12-12.18周报

## 1. 上周工作总结

### 1.1 论文研读：Darwin: Flexible Learning-based CDN Caching


Darwin的目标是实现在CDN服务器中最大化HOC命中率的准入策略，其方法是用机器学习去“检测”并从一些“已知的优秀expert”选择（expert被设计为一个元组（f,s），f代表频率阈值，s代表尺寸阈值，所有多于f次、小于s的物体都会被传输到HOC中）。Darwin最终学习将到达工作负载中的流量模式和表现最好的expert相联系，Darwin可以轻松扩展为包括其他不同于f和s的knob。

Darwin大致分为离线学习和在线选择两个步骤。

**离线学习：**

1. **离线聚类和专家集关联**：收集CDN服务器运行的历史流量轨迹，每个轨迹包括大量请求；从轨迹中提取feature（平均请求物体大小、到达间隔时间的向量和堆栈距离的向量）；根据feature将轨迹聚类；将一个命中率在表现最好的expert的θ%以内的expert与轨迹相关联；遍历每个聚类中的每个轨迹，得到一个feature到对应专家集的映射

2. **离线跨专家预测器**：映射到一个聚类的多个expert是相关联的，表明可以根据给定expert的表现预测其他expert的表现；为每对有序的expert训练神经网络，input：轨迹，output：给定另一个expert的命中/未命中次数时一个expert的命中次数的概率；预测器将被用于在线阶段

**在线学习**：以Ne个请求为周期，包括特征估计、最佳专家识别和部署

1. **特征估计**：前Nwarm-up个请求用来将当前流量和一个专家集相关联

2. **最佳专家识别和部署**：根据跨专家预测器提供的侧面信息，从这个专家集中用最佳臂识别算法选择一个最佳专家；最佳专家被部署

## 2. 下周任务

1. 继续学习本篇论文的数学推导

2. 继续自学与内容分发网络、多臂老虎机相关的文献和知识


# 12.12-12.28周报

## 1. 上周工作总结

### 1.1 论文研读：Learning Cache Replacement with CACHEUS


机器学习的发展为解决计算机科学中的问题开辟了一条新的途径，在存储领域，缓存替换策略就是之一。

现有的缓存替换策略存在以下两个问题：

1) 不同的工作负载、乃至同一个工作负载在不同的时间段会表现出不同的访存特性，对于特定工作负载缓存替换策略不一定适合另一些。
2) 缓存块的大小会影响缓存替换策略的效果。

本篇论文使用机器学习算法改进缓存替换策略，主要内容如下：

#### 1.1.1 定义了四种工作负载原语类型，任何工作负载均有这四种原语组合而成。

1) **LFU-friendly**：由访问序列定义，该序列由最近使用的最少的（LRU）缓存算法处理最优。
2) **LRU-friendly**：由访问序列定义，该序列由最不经常使用（LFU）缓存算法处理最优。
3) **scan**：由访问序列定义，每个项只被访问一次。
4) **churn**：定义为对存储项子集的重复访问，每个项被访问的概率相等。

#### 1.1.2 改进了LeCaR，提出CACHEUS。
##### 1.1.2.1 将LeCaR中的 LRU 替换为 SR-LRU，将LeCaR中的 LFU 替换为 CR-LFU

1) 通过历史预测信息更新 LRU 和 LFU 的权重，控制对 SR-LRU 和 CR-LFU 的动态使用

2) **SR-LRU**:

    1) **LRU Problem**：scan型负载会替换掉cache中被访问多次的page

    2) 它将缓存分为两部分：一部分仅包含具有多个访问权限的项（R），另一部分用于单访问项以及具有多个访问权限的旧项（SR）。SR分区允许SR-LRU具有抗扫描性；一个用于容纳新项目的分区，以便它们不会影响R中的重要项目。SR-LRU仅从SR分区逐出-当缓存已满时，它在缓存未命中时逐出SR的LRU项目。R中较旧的项被降级为SR，以便只保留在R中重用的重要项。此外，SR-LRU维护的历史列表的大小与包含最近逐出的项的缓存的大小相同。

    3) 当缓存遗漏时，x不在历史列表中，x会被插入到SR的MRU位置，如果缓存满了，SR的LRU项会被驱逐到HLU。如果缓存满了，SR的LRU项会被驱逐到H，如果H满了，算法会删除H的LRU项以腾出空间。缓存遗漏时，x在H中，x被移到R的MRU位置；缓存命中时，x在SR中，x被移到R的MRU位置；缓存命中时，x在R中，x被移到R的MRU位置。

3) **CR-LFU**:

    1) **LFU Problem**：如果要访问的项目数大于缓存的大小，则LRU风格的算法将导致缓存内容的搅动，从而重复地将项目插入缓存并从缓存中逐出

    2) 抗搅扰LFU（CR-LFU）修改了纯LFU中的替换机制，当多个缓存项的访问频率最低时，选择MRU（Most Recently Used）项来打破束缚。通过选择MRU项，CR-LFU有效地将频率最低的项目子集 “锁定 “到缓存中，使 缓存算法产生命中。

##### 1.1.2.2 采用自适应的动态学习速率

对于每一轮迭代，采取如下操作：

1) 若学习速率改变
    1) 性能改变
        1) 变优，强化变化趋势
        2) 变差，反转变化趋势
2) 若学习速率未改变
    1) 性能改变
        1) 变优，不更新
        2) 变差，随机条约
3) 如果学习速率不变，并且连续10个窗口大小性能持续下降或变为零，将学习速率重置为初始值


## 2. 下周任务

1. 准备写开题报告

2. 继续了解相关内容
